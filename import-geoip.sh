#!/bin/bash
set -e
set -o pipefail

# ============================================================
# cf-doh GeoIP Import Script
# Data Source: Loyalsoldier/geoip (Country.mmdb)
# ============================================================

: "${WORKERS_DEV:=true}"

# Cloudflare credentials are still required for D1 operations
if [ -z "$CLOUDFLARE_ACCOUNT_ID" ] || [ -z "$CLOUDFLARE_API_TOKEN" ]; then
    echo "Error: Please set CLOUDFLARE_ACCOUNT_ID and CLOUDFLARE_API_TOKEN"
    exit 1
fi

# Prepare workspace
rm -rf tmp
mkdir -p tmp
cd tmp

database_filename=geoip.db
merged_ipv4_table="merged_ipv4_data"
merged_ipv6_table="merged_ipv6_data"

# ============================================================
# Step 1: Download Country.mmdb from Loyalsoldier
# ============================================================
echo "Downloading Country.mmdb from Loyalsoldier..."
MMDB_URL="https://raw.githubusercontent.com/Loyalsoldier/geoip/release/Country.mmdb"
wget -q -O Country.mmdb "$MMDB_URL" || curl -sSfL -o Country.mmdb "$MMDB_URL"

# Get version from release date (YYYYMMDD format from Loyalsoldier tags)
database_version=$(date +%Y%m%d)
echo "Database version: $database_version"

# ============================================================
# Step 2: Extract data using Python script
# ============================================================
echo "Extracting data from MMDB..."
pip install -q maxminddb
python3 ../extract_mmdb.py Country.mmdb blocks_ipv4.csv blocks_ipv6.csv

# ============================================================
# Step 3: Create SQLite database
# ============================================================
echo "Creating SQLite database..."

# Create IPv4 table
sqlite3 $database_filename <<EOF
CREATE TABLE ${merged_ipv4_table} (
    network_start INTEGER,
    country_iso_code TEXT,
    network TEXT
);
CREATE INDEX idx_ipv4_network_start ON ${merged_ipv4_table} (network_start);
.mode csv
.import blocks_ipv4.csv ${merged_ipv4_table}_import
INSERT INTO ${merged_ipv4_table} (network_start, country_iso_code, network)
SELECT network_start, country_iso_code, network FROM ${merged_ipv4_table}_import WHERE network_start != 'network_start';
DROP TABLE ${merged_ipv4_table}_import;
EOF

# Create IPv6 table
sqlite3 $database_filename <<EOF
CREATE TABLE ${merged_ipv6_table} (
    network_start TEXT,
    country_iso_code TEXT,
    network TEXT
);
CREATE INDEX idx_ipv6_network_start ON ${merged_ipv6_table} (network_start);
.mode csv
.import blocks_ipv6.csv ${merged_ipv6_table}_import
INSERT INTO ${merged_ipv6_table} (network_start, country_iso_code, network)
SELECT network_start, country_iso_code, network FROM ${merged_ipv6_table}_import WHERE network_start != 'network_start';
DROP TABLE ${merged_ipv6_table}_import;
EOF

echo "IPv4 records: $(sqlite3 $database_filename "SELECT COUNT(*) FROM ${merged_ipv4_table};")"
echo "IPv6 records: $(sqlite3 $database_filename "SELECT COUNT(*) FROM ${merged_ipv6_table};")"

# ============================================================
# Step 4: Generate Data Migration File
# ============================================================
echo "Generating data migration..."
migration_file="../migrations/0002_data_${database_version}.sql"

{
    echo "-- GeoIP Data Import ${database_version}"
    echo "-- Auto-generated by import-geoip.sh"
    echo ""
    echo "-- Clear existing data"
    echo "DELETE FROM ${merged_ipv4_table};"
    echo "DELETE FROM ${merged_ipv6_table};"
    echo ""
    echo "-- Import IPv4 data"
    sqlite3 $database_filename ".dump ${merged_ipv4_table}" | grep "^INSERT"
    echo ""
    echo "-- Import IPv6 data"
    sqlite3 $database_filename ".dump ${merged_ipv6_table}" | grep "^INSERT"
} > $migration_file

echo "Created migration: $migration_file"

# ============================================================
# Step 5: Create/Update D1 Database
# ============================================================
database="geolite2-country"
echo "Ensuring D1 database exists: $database"

# Create database if it doesn't exist (idempotent)
npx wrangler d1 create $database --location=weur 2>/dev/null || echo "Database already exists or creation skipped"

# Get database ID
database_id=$(npx wrangler d1 list --json | jq -r ".[] | select(.name==\"$database\") | .uuid")

if [ -z "$database_id" ]; then
    echo "Error: Could not find database ID for $database"
    exit 1
fi

echo "Database ID: $database_id"

# Enable read replication
echo "Enabling read replication..."
curl -sS -X PUT "https://api.cloudflare.com/client/v4/accounts/$CLOUDFLARE_ACCOUNT_ID/d1/database/$database_id" \
  -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"read_replication": {"mode": "auto"}}' > /dev/null

# ============================================================
# Step 6: Apply Migrations
# ============================================================
echo "Applying migrations..."
cd ..
npx wrangler d1 migrations apply $database --remote -y

# Update wrangler.toml with database ID
echo "Generating wrangler.toml..."
cd tmp
sed -e "s/database_id = \".*\"/database_id = \"$database_id\"/" \
    -e "s/^workers_dev =.*/workers_dev = $WORKERS_DEV/" \
    ../wrangler.template.toml > wrangler.toml

# ============================================================
# Step 7: Cleanup old migration files (keep last 3 data migrations)
# ============================================================
echo "Cleaning up old data migrations..."
cd ../migrations
ls -1 0002_data_*.sql 2>/dev/null | head -n -3 | xargs -r rm -f || true
cd ../tmp

echo "Import complete!"
